#!/usr/bin/env python3
"""Web-of-Science query parser unit tests."""

import unittest
from search_query.parser_wos import WOSParser

class TestWOSParser(unittest.TestCase):
    """
    Unit tests for the WOSParser class.

    TestWOSParser is a unittest.TestCase subclass that tests 
    the functionality of the WOSParser class.
    It includes the following test methods:

    - setUp: Initializes the test case with a sample query string and a WOSParser instance.
    - test_tokenize: Tests the tokenization of a simple query string.
    - test_tokenize_with_combined_terms: 
        Tests the tokenization of a query string with combined terms.
    - test_tokenize_with_special_characters: 
        Tests the tokenization of a query string with special characters.

    Each test method verifies that the tokens generated by the WOSParser match the expected tokens.
    """

    def setUp(self):
        self.query_str = "TI=example AND AU=John Doe"
        self.parser = WOSParser(query_str=self.query_str, search_fields="", mode="")

    def test_tokenize(self):
        """
        Test the tokenize method of the parser.

        This test verifies that the tokenize method correctly tokenizes
        a given input string into the expected list of tokens with their
        respective positions.

        The expected tokens are:
        - ("TI=", (0, 3))
        - ("example", (3, 10))
        - ("AND", (11, 14))
        - ("AU=", (15, 18))
        - ("John Doe", (18, 26))

        Asserts that the parser's tokens match the expected tokens.
        """
        self.parser.tokenize()
        expected_tokens = [
            ("TI=", (0, 3)),
            ("example", (3, 10)),
            ("AND", (11, 14)),
            ("AU=", (15, 18)),
            ("John Doe", (18, 26))
        ]
        self.assertEqual(self.parser.tokens, expected_tokens)

    def test_tokenize_with_combined_terms(self):
        """
        Test the `tokenize` method of the parser with a query string that contains combined terms.

        This test sets the `query_str` attribute of the parser to a string with combined terms
        and logical operators, then calls the `tokenize` method. It verifies that the tokens
        generated by the `tokenize` method match the expected tokens.

        The query string used in this test is:
            "TI=example example2 AND AU=John Doe"

        Asserts:
            self.assertEqual(self.parser.tokens, expected_tokens): Checks if the tokens
            generated by the `tokenize` method match the expected tokens.
        """
        self.parser.query_str = "TI=example example2 AND AU=John Doe"
        self.parser.tokenize()
        expected_tokens = [
            ("TI=", (0, 3)),
            ("example example2", (3, 19)),
            ("AND", (20, 23)),
            ("AU=", (24, 27)),
            ("John Doe", (27, 35))
        ]
        self.assertEqual(self.parser.tokens, expected_tokens)

    def test_tokenize_with_multiple_combined_terms(self):
        """
        Test the `tokenize` method of the parser with a query string that contains combined terms.

        This test sets the `query_str` attribute of the parser to a string with combined terms
        and logical operators, then calls the `tokenize` method. It verifies that the tokens
        generated by the `tokenize` method match the expected tokens.

        The query string used in this test is:
            "TI=example example2 example3 AND AU=John Doe"

        The expected tokens are:
            [

        Asserts:
            self.assertEqual(self.parser.tokens, expected_tokens): Checks if the tokens
            generated by the `tokenize` method match the expected tokens.
        """
        self.parser.query_str = "TI=example example2 example3 AND AU=John Doe"
        self.parser.tokenize()
        expected_tokens = [
            ("TI=", (0, 3)),
            ("example example2 example3", (3, 28)),
            ("AND", (29, 32)),
            ("AU=", (33, 36)),
            ("John Doe", (36, 44))
        ]
        self.assertEqual(self.parser.tokens, expected_tokens)

    def test_tokenize_with_special_characters(self):
        """
        Test the `tokenize` method of the parser with a query string containing special characters.

        This test sets the `query_str` attribute of the parser to a string with special characters
        and calls the `tokenize` method. It then checks if the tokens generated by the `tokenize`
        method match the expected tokens.

        The query string used in this test is:
        "TI=ex$mple* AND AU=John?Doe"

        The expected tokens are:
        [

        Asserts:
            self.assertEqual(self.parser.tokens, expected_tokens): Checks if the tokens generated
            by the `tokenize` method match the expected tokens.
        """
        self.parser.query_str = "TI=ex$mple* AND AU=John?Doe"
        self.parser.tokenize()
        expected_tokens = [
            ("TI=", (0, 3)),
            ("ex$mple*", (3, 11)),
            ("AND", (12, 15)),
            ("AU=", (16, 19)),
            ("John?Doe", (19, 27))
        ]
        self.assertEqual(self.parser.tokens, expected_tokens)

if __name__ == '__main__':
    unittest.main()
